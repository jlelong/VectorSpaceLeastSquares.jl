# VectorSpaceLeastSquares

[![Build Status](https://github.com/jlelong/VectorSpaceLeastSquares.jl/actions/workflows/CI.yml/badge.svg?branch=main)](https://github.com/jlelong/VectorSpaceLeastSquares.jl/actions/workflows/CI.yml?query=branch%3Amain)

This package aims at computing the least squares approximation within a vector space of function

$$\inf_{\alpha \in \mathbb{R}^d} \sum_{m=1}^M \left(\sum_{i=1}^d \alpha_i g_i\circ\varphi(x_m) - y_m\right)^2$$

where

- $(x_m, y_m)_{1 \le m \le M}$ are training values: $y_m \in \mathbb{R}$ is the expected value at point $x_m \in \mathbb{R}^d$.
- $\varphi: \mathbb{R}^d \to \mathbb{R}^d$ is a transformation to be be applied to the input data before solving the least squares problem
- For $i = 1, \dots, d$, $g_i : \mathbb{R}^d \to \mathbb{R}$ and the family $(g_1, \dots, g_d)$ is a free family. We call the family a basis in the following.

Let $\mathcal{V}$ be the vector space generated by the functions $(g_1, \dots, g_d)$, this package computes the best least squares approximation of the unknown function $x \longmapsto y$ inside $\mathcal{V}$ up to a transformation $\varphi$.

## Transformations

Transformations are are represented by the type `AbstractTransformation` and must implement the function

```julia
apply!(t::AbstractTransformation, tx::AbstractVector{<:Real}, x::AbstractVector{<:Real})
```

The function `apply!` applies the transformation to `x` and stores the result in `tx`.

Currently two transformations are implemented

- `VoidTransformation` which does nothing: $\varphi(x) = $.
- `LinearTransformation` which corresponds to $\varphi(x) = \sigma (x - \mu)$

    ```julia
    struct LinearTransformation <: AbstractTransformation
        scale::Vector{<:Real}
        center::Vector{<:Real}
    end

    function apply!(t::LinearTransformation, tx::AbstractVector{<:Real}, x::AbstractVector{<:Real})
        tx .= (x .- t.center) .* scale
    end
    ````

    If you intend to use $\varphi$ to center the data around its mean and rescale them by their standard deviation, you can construct the `LinearTransformation` using

    ```julia
    LinearTransformation(x::AbstractVector{<:AbstractVector{T}}) where T<:Real
    ```

    Example

    ```julia
    x = [randn(10) for _ in 1:10000] # Samples from X = N(0,I_{10})
    l = LinearTransformation(x)
    l.center # mean(X) ~ 0
    l.scale # 1 / stddev(X) ~ 1
    ```

A new transformation can be created by deriving a new concrete subtype of `AbstractTransformation`

```julia
struct MyTransformation <: AbstractTransformation
    field1
    field2
    ....
end

function apply!(t::MyTransformation, tx::AbstractVector{<:Real}, x::AbstractVector{<:Real})
    .....
    tx .=
end
```

## Basis

The vector space $\mathcal{V}$ is represented by the abstract type `AbstractBasis` which provides the following method

```julia
"""
Return the number of variates of the functions inside the basis
"""
nVariates(B::AbstractBasis)
"""
Return the number of elements in the basis
"""
length(B::AbstractBasis)
"""
Return the internal basis type
"""
type(B::AbstractBasis)
"""
Compute the value of the `index`-th basis function at point `x`
"""
value(B::AbstractBasis, x::AbstractVector{<:Real}, index::Integer)
"""
Compute the value of the first derivative of the `index`-th basis function w.r.t to the `derivativeIndex` variate at point `x`
"""
derivative(B::AbstractBasis, x::AbstractVector{<:Real}, index::Integer, derivativeIndex::Integer)
```

Currently polynomial functions are implemented using the `PolynomialBasis` type

```julia
@enum PolynomialType begin
    Canonic
    Hermite
    Tchebychev
end

struct PolynomialBasis <: AbstractBasis
    degree::Int64 # Total maxixmum degree
    nVariates::Int64 # Number of variates
    size::Int64 # Dimension of the vector space `V`
    type::PolynomialType
    tensor::SparseMatrixCSC{Int64, Int64} # Store polynomials by column
end

PolynomialBasis(degree::Integer, nVariates::Integer, type::PolynomialType)
```

## The least squares problem

The least squares problem is solved using the following interface

```julia
struct VSLeastSquares{Tb<:AbstractBasis, Tt<:AbstractTransformation, Td<:Real}
    basis::Tb
    transformation::Tt
    coefficients::Vector{Td} # Available once fit is called
    _transformed_data::Vector{Td} # Internal usage only to store `\varphi(x)`
end

length(vslsq::VSLeastSquares{Tb, Tt, Td}) where {Tb<:AbstractBasis, Tt<:AbstractTransformation, Td<:Real}
getCoefficients(vslsq::VSLeastSquares{Tb, Tt, Td}) where {Tb<:AbstractBasis, Tt<:AbstractTransformation, Td<:Real}
getBasis(vslsq::VSLeastSquares{Tb, Tt, Td}) where {Tb<:AbstractBasis, Tt<:AbstractTransformation, Td<:Real}

VSLeastSquares(basis::Tb, transform::Tt=VoidTransformation(), Td::Type=Float64)
```

To compute the optimal coefficients $\alpha$ of the least squares problem, use the method

```julia
fit(vslsq::VSLeastSquares{Tb, Tt, Td}, x::AbstractVector{<:AbstractVector{Td}}, y::AbstractVector{Td}) where {Tb<:AbstractBasis, Tt<:AbstractTransformation, Td<:Real}
```

The coefficients are stored in `VSLeastSquares.coefficients` and can be retrieved with `getCoefficients`.

Once a model is fit, a prediction can be obtained using

```julia
predict(vslsq::VSLeastSquares{Tb, Tt, Td}, x::AbstractVector{Td}) where {Tb<:AbstractBasis, Tt<:AbstractTransformation, Td<:Real}
```

## Some examples

```julia
dim = 4
deg = 3
nSamples = 50000
T = Float32
f(x) = 2 * x[2]^3 - x[1] * x[4] + 7 * x[3]^2 * x[1]
data = [randn(T, dim) for i in 1:nSamples]
y = f.(data)
vslsq = VSLeastSquares(PolynomialBasis(deg, dim, Hermite), VoidTransformation(), T)
fit(vslsq, data, y)
x = randn(T, dim)
predict(vslsq, x)
```
